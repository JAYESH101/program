{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JAYESH101/program/blob/master/Contlo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLa0SZGAi00j"
      },
      "outputs": [],
      "source": [
        "!pip install einops\n",
        "!pip install rotary-embedding-torch\n",
        "!pip install torch\n",
        "!pip install axial-positional-embedding\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtuKWnvri-6x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V28PDCsRf1VP",
        "outputId": "52e2c28f-1b72-49c8-91ba-ef41aa727420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 20, 128])\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N = query.shape[0]\n",
        "\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.nn.functional.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class GPT2Small(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, heads, depth):\n",
        "        super(GPT2Small, self).__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_embedding = nn.Embedding(1000, embed_size)\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        MultiHeadAttention(embed_size, heads),\n",
        "                        nn.LayerNorm(embed_size),\n",
        "                        nn.Linear(embed_size, 4 * embed_size),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(4 * embed_size, embed_size),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(device)\n",
        "        out = self.word_embedding(x) + self.positional_embedding(positions)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            attention = layer[0](out, out, out, mask)\n",
        "            out = out + attention\n",
        "            out = layer[1](out)\n",
        "            out = layer[4](layer[3](layer[2](out)))\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "vocab_size = 10000\n",
        "embed_size = 128\n",
        "heads = 4\n",
        "depth = 3\n",
        "model = GPT2Small(vocab_size, embed_size, heads, depth)\n",
        "\n",
        "input_sequence = torch.randint(0, vocab_size, (32, 20))\n",
        "mask = (input_sequence != 0).unsqueeze(1).unsqueeze(2)\n",
        "output = model(input_sequence, mask)\n",
        "print(output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nt3t3sFgeEU",
        "outputId": "97ca0b0d-7d6a-4f47-ae1f-788a302e7da9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text:\n",
            " Once upon a time, in a way, I felt like I was in the middle of something.\n",
            "\n",
            "\"It was like, 'Oh my God, this is going to be great.' And then I realized that I wasn't going anywhere.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "input_text = \"Once upon a time, in a\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "output_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.7,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    attention_mask=torch.ones_like(input_ids)\n",
        ")\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\\n\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36bmSY5miGat",
        "outputId": "9e23f548-9c40-4877-e120-81a72fd455b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Case 1:\n",
            " Input Prompt: Once upon a time, in a \n",
            "Generated Text: Once upon a time, in a time of war, in a time of peace, in a time of peace, in a time of peace, in a time of peace, in a time of peace, in a time of peace, in a time\n",
            "==================================================\n",
            "Test Case 2:\n",
            " Input Prompt: In a galaxy far, far away, \n",
            "Generated Text: In a galaxy far, far away, there is a galaxy far, far, far away.\n",
            "\n",
            "In a galaxy far, far away, there is a galaxy far, far, far, far, far, far, far, far, far\n",
            "==================================================\n",
            "Test Case 3:\n",
            " Input Prompt: To be or not to be, that is \n",
            "Generated Text: To be or not to be, that is not the point.\n",
            "\n",
            "The point is that if you want to be or not to be, that is not the point.\n",
            "\n",
            "The point is that if you want to be or not to be\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "def test_gpt2_small(model, tokenizer, input_text, max_length=50, num_beams=5, temperature=0.7):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        temperature=temperature,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        attention_mask=torch.ones_like(input_ids)\n",
        "    )\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "input_prompt = \"Once upon a time, in a\"\n",
        "output_text = test_gpt2_small(gpt2_model, tokenizer, input_prompt)\n",
        "print(\"Test Case 1:\\n\", \"Input Prompt:\", input_prompt, \"\\nGenerated Text:\", output_text)\n",
        "print(\"=\"*50)\n",
        "\n",
        "input_prompt = \"In a galaxy far, far away,\"\n",
        "output_text = test_gpt2_small(gpt2_model, tokenizer, input_prompt)\n",
        "print(\"Test Case 2:\\n\", \"Input Prompt:\", input_prompt, \"\\nGenerated Text:\", output_text)\n",
        "print(\"=\"*50)\n",
        "\n",
        "input_prompt = \"To be or not to be, that is\"\n",
        "output_text = test_gpt2_small(gpt2_model, tokenizer, input_prompt)\n",
        "print(\"Test Case 3:\\n\", \"Input Prompt:\", input_prompt, \"\\nGenerated Text:\", output_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMevGt7GLnem4I+xlFy1Zw6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}